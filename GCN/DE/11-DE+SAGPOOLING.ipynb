{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ec9b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "train_data = torch.load('train_data.pt')\n",
    "val_data = torch.load('val_data.pt')\n",
    "test_data = torch.load('test_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96faf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score, recall_score\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv1d, MaxPool1d, Linear, Dropout, BCEWithLogitsLoss, GRU\n",
    "\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, aggr\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80918fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "from torch_geometric.utils import k_hop_subgraph, to_scipy_sparse_matrix\n",
    "\n",
    "def seal_processing(dataset, edge_label_index, y, max_dist=6):\n",
    "    data_list = []\n",
    "    for src, dst in edge_label_index.t().tolist():\n",
    "        # 提取子图\n",
    "        sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph([src, dst], 2, dataset.edge_index, relabel_nodes=True)\n",
    "        src, dst = mapping.tolist()\n",
    "\n",
    "        # 从子图中移除目标边\n",
    "        mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "        mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "        sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "        # 确保 src < dst\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "\n",
    "        # 计算邻接矩阵\n",
    "        adj = to_scipy_sparse_matrix(sub_edge_index, num_nodes=sub_nodes.size(0)).tocsr()\n",
    "\n",
    "        # 计算距离编码\n",
    "        dist = shortest_path(adj, directed=False, unweighted=True, indices=[src, dst])\n",
    "        dist = torch.from_numpy(dist).to(torch.long)\n",
    "\n",
    "        # 处理距离编码\n",
    "        dist[dist > max_dist] = max_dist\n",
    "        dist[torch.isnan(dist)] = max_dist + 1\n",
    "\n",
    "        # 将距离转置以匹配预期的形状 (num_nodes, 2)\n",
    "        dist = dist.t()\n",
    "\n",
    "        # 对每一个距离进行 one-hot 编码\n",
    "        node_labels_src = F.one_hot(dist[:, 0], num_classes=max_dist + 2).to(torch.float)\n",
    "        node_labels_dst = F.one_hot(dist[:, 1], num_classes=max_dist + 2).to(torch.float)\n",
    "\n",
    "        # 将两个 one-hot 编码的结果拼接起来\n",
    "        node_labels = torch.cat([node_labels_src, node_labels_dst], dim=1)\n",
    "\n",
    "        # 获取子图中的节点特征\n",
    "        node_emb = dataset.x[sub_nodes]\n",
    "        \n",
    "        # 拼接节点特征和距离标签\n",
    "        node_x = torch.cat([node_emb, node_labels], dim=1)\n",
    "\n",
    "        # 创建数据对象\n",
    "        data = Data(x=node_x, z=dist, edge_index=sub_edge_index, y=y)\n",
    "        data_list.append(data)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cf531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enclosing subgraphs extraction\n",
    "train_pos_data_list = seal_processing(train_data, train_data.pos_edge_label_index, 1)\n",
    "train_neg_data_list = seal_processing(train_data, train_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "399be37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#索引边界，和seal本身有关\n",
    "#valid_mask = (val_data.neg_edge_label_index[0]) & (val_data.neg_edge_label_index[1])\n",
    "#val_data.neg_edge_label_index = val_data.neg_edge_label_index[:, valid_mask]\n",
    "val_pos_data_list = seal_processing(val_data, val_data.pos_edge_label_index, 1)\n",
    "val_neg_data_list = seal_processing(val_data, val_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90731ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_mask = (test_data.neg_edge_label_index[0]) & (test_data.neg_edge_label_index[1])\n",
    "#test_data.neg_edge_label_index = test_data.neg_edge_label_index[:, test_mask]\n",
    "test_pos_data_list = seal_processing(test_data, test_data.pos_edge_label_index, 1)\n",
    "test_neg_data_list = seal_processing(test_data, test_data.neg_edge_label_index, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d337ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_pos_data_list + train_neg_data_list\n",
    "val_dataset = val_pos_data_list + val_neg_data_list\n",
    "test_dataset = test_pos_data_list + test_neg_data_list\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8915ac54-27c2-4715-9863-9778cad1d3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Dropout, Conv1d, MaxPool1d\n",
    "from torch.nn.functional import relu, sigmoid\n",
    "from torch_geometric.nn import GCNConv, SAGPooling\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Dropout, Conv1d, MaxPool1d\n",
    "from torch.nn.functional import relu, sigmoid\n",
    "from torch_geometric.nn import GCNConv, SAGPooling, global_mean_pool\n",
    "\n",
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, dim_in, k=0.5, ratio=0.5):  # k和ratio是SAGPooling的参数\n",
    "        super().__init__()\n",
    "\n",
    "        # GCN layers\n",
    "        self.gcn1 = GCNConv(dim_in, 32)\n",
    "        self.gcn2 = GCNConv(32, 32)\n",
    "        self.gcn3 = GCNConv(32, 32)\n",
    "        self.gcn4 = GCNConv(32, 1)\n",
    "\n",
    "        # Self-Attention Graph Pooling\n",
    "        self.global_pool = SAGPooling(in_channels=97, ratio=ratio)  # 这里假设concat后的特征维度为97\n",
    "\n",
    "        # Dense layers\n",
    "        self.linear1 = Linear(97, 32)  # 注意这里的输入维度可能需要根据SAGPooling的结果调整\n",
    "        self.dropout = Dropout(0.5)\n",
    "        self.linear2 = Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Graph Convolutional Layers\n",
    "        h1 = self.gcn1(x, edge_index).tanh()\n",
    "        h2 = self.gcn2(h1, edge_index).tanh()\n",
    "        h3 = self.gcn3(h2, edge_index).tanh()\n",
    "        h4 = self.gcn4(h3, edge_index).tanh()\n",
    "        h = torch.cat([h1, h2, h3, h4], dim=-1)\n",
    "\n",
    "        # 2. Self-Attention Graph Pooling\n",
    "        h, _, _, batch, _, _ = self.global_pool(h, edge_index, batch=batch)  # 返回值包括节点特征、边索引、batch索引等\n",
    "        h = h.view(h.size(0), -1)  # 将结果展平\n",
    "\n",
    "        # 3. Global mean pooling to get graph-level representation\n",
    "        h = global_mean_pool(h, batch)  # 每个图的节点特征聚合为一个向量\n",
    "\n",
    "        # 4. Dense layers\n",
    "        h = self.linear1(h).relu()\n",
    "        h = self.dropout(h)\n",
    "        h = self.linear2(h).sigmoid()\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798ab952",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(train_dataset[0].num_features).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "criterion = BCEWithLogitsLoss()\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = criterion(out.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += float(loss) * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(out.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    y_pred_binary = (torch.cat(y_pred) > 0.5).numpy()\n",
    "    y_true_array = torch.cat(y_true).numpy()\n",
    "\n",
    "    auc = roc_auc_score(y_true_array, torch.cat(y_pred))\n",
    "    accuracy = accuracy_score(y_true_array, y_pred_binary)\n",
    "    f1 = f1_score(y_true_array, y_pred_binary)\n",
    "    precision = precision_score(y_true_array, y_pred_binary)\n",
    "    recall = recall_score(y_true_array, y_pred_binary)\n",
    "\n",
    "    return auc, accuracy, f1, precision, recall\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8b665e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 | Loss: 0.6624 | Val AUC: 0.6850 \n",
      "Epoch  1 | Loss: 0.5588 | Val AUC: 0.6905 \n",
      "Epoch  2 | Loss: 0.5498 | Val AUC: 0.6917 \n",
      "Epoch  3 | Loss: 0.5465 | Val AUC: 0.6908 \n",
      "Epoch  4 | Loss: 0.5450 | Val AUC: 0.6906 \n",
      "Epoch  5 | Loss: 0.5439 | Val AUC: 0.6878 \n",
      "Epoch  6 | Loss: 0.5433 | Val AUC: 0.6851 \n",
      "Epoch  7 | Loss: 0.5432 | Val AUC: 0.6849 \n",
      "Epoch  8 | Loss: 0.5433 | Val AUC: 0.6823 \n",
      "Epoch  9 | Loss: 0.5423 | Val AUC: 0.6841 \n",
      "Epoch 10 | Loss: 0.5425 | Val AUC: 0.6841 \n",
      "Epoch 11 | Loss: 0.5423 | Val AUC: 0.6851 \n",
      "Epoch 12 | Loss: 0.5418 | Val AUC: 0.6899 \n",
      "Epoch 13 | Loss: 0.5415 | Val AUC: 0.6898 \n",
      "Epoch 14 | Loss: 0.5410 | Val AUC: 0.6895 \n",
      "Epoch 15 | Loss: 0.5411 | Val AUC: 0.6922 \n",
      "Epoch 16 | Loss: 0.5410 | Val AUC: 0.6918 \n",
      "Epoch 17 | Loss: 0.5415 | Val AUC: 0.6921 \n",
      "Epoch 18 | Loss: 0.5408 | Val AUC: 0.6970 \n",
      "Epoch 19 | Loss: 0.5406 | Val AUC: 0.6961 \n",
      "Epoch 20 | Loss: 0.5412 | Val AUC: 0.6990 \n",
      "Epoch 21 | Loss: 0.5405 | Val AUC: 0.6977 \n",
      "Epoch 22 | Loss: 0.5406 | Val AUC: 0.6983 \n",
      "Epoch 23 | Loss: 0.5405 | Val AUC: 0.7015 \n",
      "Epoch 24 | Loss: 0.5409 | Val AUC: 0.7008 \n",
      "Epoch 25 | Loss: 0.5402 | Val AUC: 0.7006 \n",
      "Epoch 26 | Loss: 0.5404 | Val AUC: 0.7083 \n",
      "Epoch 27 | Loss: 0.5406 | Val AUC: 0.7100 \n",
      "Epoch 28 | Loss: 0.5403 | Val AUC: 0.7042 \n",
      "Epoch 29 | Loss: 0.5401 | Val AUC: 0.7041 \n",
      "Epoch 30 | Loss: 0.5399 | Val AUC: 0.7146 \n",
      "Epoch 31 | Loss: 0.5399 | Val AUC: 0.7135 \n",
      "Epoch 32 | Loss: 0.5404 | Val AUC: 0.7142 \n",
      "Epoch 33 | Loss: 0.5404 | Val AUC: 0.7162 \n",
      "Epoch 34 | Loss: 0.5401 | Val AUC: 0.7156 \n",
      "Epoch 35 | Loss: 0.5398 | Val AUC: 0.7163 \n",
      "Epoch 36 | Loss: 0.5397 | Val AUC: 0.7172 \n",
      "Epoch 37 | Loss: 0.5397 | Val AUC: 0.7188 \n",
      "Epoch 38 | Loss: 0.5397 | Val AUC: 0.7206 \n",
      "Epoch 39 | Loss: 0.5395 | Val AUC: 0.7229 \n",
      "Epoch 40 | Loss: 0.5392 | Val AUC: 0.7227 \n",
      "Epoch 41 | Loss: 0.5393 | Val AUC: 0.7236 \n",
      "Epoch 42 | Loss: 0.5393 | Val AUC: 0.7255 \n",
      "Epoch 43 | Loss: 0.5393 | Val AUC: 0.7269 \n",
      "Epoch 44 | Loss: 0.5392 | Val AUC: 0.7283 \n",
      "Epoch 45 | Loss: 0.5391 | Val AUC: 0.7280 \n",
      "Epoch 46 | Loss: 0.5388 | Val AUC: 0.7278 \n",
      "Epoch 47 | Loss: 0.5386 | Val AUC: 0.7285 \n",
      "Epoch 48 | Loss: 0.5388 | Val AUC: 0.7289 \n",
      "Epoch 49 | Loss: 0.5386 | Val AUC: 0.7298 \n",
      "Epoch 50 | Loss: 0.5387 | Val AUC: 0.7280 \n",
      "Epoch 51 | Loss: 0.5386 | Val AUC: 0.7307 \n",
      "Epoch 52 | Loss: 0.5382 | Val AUC: 0.7313 \n",
      "Epoch 53 | Loss: 0.5385 | Val AUC: 0.7313 \n",
      "Epoch 54 | Loss: 0.5382 | Val AUC: 0.7321 \n",
      "Epoch 55 | Loss: 0.5381 | Val AUC: 0.7313 \n",
      "Epoch 56 | Loss: 0.5384 | Val AUC: 0.7320 \n",
      "Epoch 57 | Loss: 0.5380 | Val AUC: 0.7326 \n",
      "Epoch 58 | Loss: 0.5380 | Val AUC: 0.7311 \n",
      "Epoch 59 | Loss: 0.5377 | Val AUC: 0.7319 \n",
      "Epoch 60 | Loss: 0.5374 | Val AUC: 0.7334 \n",
      "Epoch 61 | Loss: 0.5375 | Val AUC: 0.7319 \n",
      "Epoch 62 | Loss: 0.5372 | Val AUC: 0.7326 \n",
      "Epoch 63 | Loss: 0.5375 | Val AUC: 0.7325 \n",
      "Epoch 64 | Loss: 0.5373 | Val AUC: 0.7311 \n",
      "Epoch 65 | Loss: 0.5372 | Val AUC: 0.7326 \n",
      "Epoch 66 | Loss: 0.5367 | Val AUC: 0.7325 \n",
      "Epoch 67 | Loss: 0.5367 | Val AUC: 0.7331 \n",
      "Epoch 68 | Loss: 0.5366 | Val AUC: 0.7335 \n",
      "Epoch 69 | Loss: 0.5362 | Val AUC: 0.7331 \n",
      "Epoch 70 | Loss: 0.5366 | Val AUC: 0.7330 \n",
      "Epoch 71 | Loss: 0.5362 | Val AUC: 0.7334 \n",
      "Epoch 72 | Loss: 0.5369 | Val AUC: 0.7328 \n",
      "Epoch 73 | Loss: 0.5366 | Val AUC: 0.7325 \n",
      "Epoch 74 | Loss: 0.5361 | Val AUC: 0.7323 \n",
      "Epoch 75 | Loss: 0.5362 | Val AUC: 0.7325 \n",
      "Epoch 76 | Loss: 0.5362 | Val AUC: 0.7330 \n",
      "Epoch 77 | Loss: 0.5360 | Val AUC: 0.7325 \n",
      "Epoch 78 | Loss: 0.5363 | Val AUC: 0.7330 \n",
      "Epoch 79 | Loss: 0.5358 | Val AUC: 0.7324 \n",
      "Epoch 80 | Loss: 0.5361 | Val AUC: 0.7324 \n",
      "Epoch 81 | Loss: 0.5358 | Val AUC: 0.7332 \n",
      "Epoch 82 | Loss: 0.5357 | Val AUC: 0.7322 \n",
      "Epoch 83 | Loss: 0.5357 | Val AUC: 0.7320 \n",
      "Epoch 84 | Loss: 0.5354 | Val AUC: 0.7317 \n",
      "Epoch 85 | Loss: 0.5358 | Val AUC: 0.7320 \n",
      "Epoch 86 | Loss: 0.5356 | Val AUC: 0.7315 \n",
      "Epoch 87 | Loss: 0.5360 | Val AUC: 0.7313 \n",
      "Epoch 88 | Loss: 0.5358 | Val AUC: 0.7307 \n",
      "Epoch 89 | Loss: 0.5357 | Val AUC: 0.7309 \n",
      "Epoch 90 | Loss: 0.5351 | Val AUC: 0.7316 \n",
      "Epoch 91 | Loss: 0.5351 | Val AUC: 0.7312 \n",
      "Epoch 92 | Loss: 0.5354 | Val AUC: 0.7316 \n",
      "Epoch 93 | Loss: 0.5354 | Val AUC: 0.7328 \n",
      "Epoch 94 | Loss: 0.5355 | Val AUC: 0.7333 \n",
      "Epoch 95 | Loss: 0.5352 | Val AUC: 0.7317 \n",
      "Epoch 96 | Loss: 0.5354 | Val AUC: 0.7318 \n",
      "Epoch 97 | Loss: 0.5354 | Val AUC: 0.7317 \n",
      "Epoch 98 | Loss: 0.5355 | Val AUC: 0.7314 \n",
      "Epoch 99 | Loss: 0.5351 | Val AUC: 0.7309 \n",
      "Epoch 100 | Loss: 0.5354 | Val AUC: 0.7309 \n",
      "Epoch 101 | Loss: 0.5352 | Val AUC: 0.7304 \n",
      "Epoch 102 | Loss: 0.5351 | Val AUC: 0.7310 \n",
      "Epoch 103 | Loss: 0.5353 | Val AUC: 0.7302 \n",
      "Epoch 104 | Loss: 0.5352 | Val AUC: 0.7303 \n",
      "Epoch 105 | Loss: 0.5355 | Val AUC: 0.7303 \n",
      "Epoch 106 | Loss: 0.5352 | Val AUC: 0.7299 \n",
      "Epoch 107 | Loss: 0.5351 | Val AUC: 0.7304 \n",
      "Epoch 108 | Loss: 0.5351 | Val AUC: 0.7304 \n",
      "Epoch 109 | Loss: 0.5349 | Val AUC: 0.7298 \n",
      "Epoch 110 | Loss: 0.5352 | Val AUC: 0.7305 \n",
      "Epoch 111 | Loss: 0.5351 | Val AUC: 0.7310 \n",
      "Epoch 112 | Loss: 0.5346 | Val AUC: 0.7308 \n",
      "Epoch 113 | Loss: 0.5352 | Val AUC: 0.7313 \n",
      "Epoch 114 | Loss: 0.5349 | Val AUC: 0.7307 \n",
      "Epoch 115 | Loss: 0.5348 | Val AUC: 0.7301 \n",
      "Epoch 116 | Loss: 0.5350 | Val AUC: 0.7307 \n",
      "Epoch 117 | Loss: 0.5351 | Val AUC: 0.7299 \n",
      "Epoch 118 | Loss: 0.5359 | Val AUC: 0.7298 \n",
      "Epoch 119 | Loss: 0.5356 | Val AUC: 0.7305 \n",
      "Epoch 120 | Loss: 0.5351 | Val AUC: 0.7292 \n",
      "Epoch 121 | Loss: 0.5351 | Val AUC: 0.7301 \n",
      "Epoch 122 | Loss: 0.5351 | Val AUC: 0.7302 \n",
      "Epoch 123 | Loss: 0.5347 | Val AUC: 0.7301 \n",
      "Epoch 124 | Loss: 0.5351 | Val AUC: 0.7302 \n",
      "Epoch 125 | Loss: 0.5349 | Val AUC: 0.7298 \n",
      "Epoch 126 | Loss: 0.5349 | Val AUC: 0.7302 \n",
      "Epoch 127 | Loss: 0.5347 | Val AUC: 0.7303 \n",
      "Epoch 128 | Loss: 0.5353 | Val AUC: 0.7302 \n",
      "Epoch 129 | Loss: 0.5347 | Val AUC: 0.7301 \n",
      "Epoch 130 | Loss: 0.5347 | Val AUC: 0.7299 \n",
      "Epoch 131 | Loss: 0.5345 | Val AUC: 0.7299 \n",
      "Epoch 132 | Loss: 0.5350 | Val AUC: 0.7292 \n",
      "Epoch 133 | Loss: 0.5348 | Val AUC: 0.7293 \n",
      "Epoch 134 | Loss: 0.5349 | Val AUC: 0.7293 \n",
      "Epoch 135 | Loss: 0.5347 | Val AUC: 0.7285 \n",
      "Epoch 136 | Loss: 0.5346 | Val AUC: 0.7289 \n",
      "Epoch 137 | Loss: 0.5347 | Val AUC: 0.7296 \n",
      "Epoch 138 | Loss: 0.5348 | Val AUC: 0.7289 \n",
      "Epoch 139 | Loss: 0.5346 | Val AUC: 0.7274 \n",
      "Epoch 140 | Loss: 0.5346 | Val AUC: 0.7277 \n",
      "Epoch 141 | Loss: 0.5350 | Val AUC: 0.7288 \n",
      "Epoch 142 | Loss: 0.5346 | Val AUC: 0.7283 \n",
      "Epoch 143 | Loss: 0.5346 | Val AUC: 0.7288 \n",
      "Epoch 144 | Loss: 0.5344 | Val AUC: 0.7298 \n",
      "Epoch 145 | Loss: 0.5352 | Val AUC: 0.7288 \n",
      "Epoch 146 | Loss: 0.5349 | Val AUC: 0.7300 \n",
      "Epoch 147 | Loss: 0.5346 | Val AUC: 0.7301 \n",
      "Epoch 148 | Loss: 0.5346 | Val AUC: 0.7305 \n",
      "Epoch 149 | Loss: 0.5346 | Val AUC: 0.7291 \n",
      "Epoch 150 | Loss: 0.5349 | Val AUC: 0.7313 \n",
      "Epoch 151 | Loss: 0.5350 | Val AUC: 0.7319 \n",
      "Epoch 152 | Loss: 0.5340 | Val AUC: 0.7318 \n",
      "Epoch 153 | Loss: 0.5343 | Val AUC: 0.7314 \n",
      "Epoch 154 | Loss: 0.5340 | Val AUC: 0.7309 \n",
      "Epoch 155 | Loss: 0.5345 | Val AUC: 0.7302 \n",
      "Epoch 156 | Loss: 0.5342 | Val AUC: 0.7305 \n",
      "Epoch 157 | Loss: 0.5342 | Val AUC: 0.7290 \n",
      "Epoch 158 | Loss: 0.5345 | Val AUC: 0.7295 \n",
      "Epoch 159 | Loss: 0.5343 | Val AUC: 0.7296 \n",
      "Epoch 160 | Loss: 0.5344 | Val AUC: 0.7278 \n",
      "Epoch 161 | Loss: 0.5342 | Val AUC: 0.7275 \n",
      "Epoch 162 | Loss: 0.5341 | Val AUC: 0.7281 \n",
      "Epoch 163 | Loss: 0.5340 | Val AUC: 0.7280 \n",
      "Epoch 164 | Loss: 0.5346 | Val AUC: 0.7268 \n",
      "Epoch 165 | Loss: 0.5343 | Val AUC: 0.7259 \n",
      "Epoch 166 | Loss: 0.5341 | Val AUC: 0.7256 \n",
      "Epoch 167 | Loss: 0.5342 | Val AUC: 0.7261 \n",
      "Epoch 168 | Loss: 0.5341 | Val AUC: 0.7254 \n",
      "Epoch 169 | Loss: 0.5342 | Val AUC: 0.7258 \n",
      "Epoch 170 | Loss: 0.5339 | Val AUC: 0.7269 \n",
      "Epoch 171 | Loss: 0.5341 | Val AUC: 0.7265 \n",
      "Epoch 172 | Loss: 0.5346 | Val AUC: 0.7245 \n",
      "Epoch 173 | Loss: 0.5339 | Val AUC: 0.7256 \n",
      "Epoch 174 | Loss: 0.5337 | Val AUC: 0.7255 \n",
      "Epoch 175 | Loss: 0.5338 | Val AUC: 0.7247 \n",
      "Epoch 176 | Loss: 0.5337 | Val AUC: 0.7249 \n",
      "Epoch 177 | Loss: 0.5341 | Val AUC: 0.7249 \n",
      "Epoch 178 | Loss: 0.5345 | Val AUC: 0.7251 \n",
      "Epoch 179 | Loss: 0.5339 | Val AUC: 0.7247 \n",
      "Epoch 180 | Loss: 0.5337 | Val AUC: 0.7241 \n",
      "Epoch 181 | Loss: 0.5337 | Val AUC: 0.7268 \n",
      "Epoch 182 | Loss: 0.5338 | Val AUC: 0.7238 \n",
      "Epoch 183 | Loss: 0.5339 | Val AUC: 0.7249 \n",
      "Epoch 184 | Loss: 0.5341 | Val AUC: 0.7257 \n",
      "Epoch 185 | Loss: 0.5344 | Val AUC: 0.7261 \n",
      "Epoch 186 | Loss: 0.5346 | Val AUC: 0.7247 \n",
      "Epoch 187 | Loss: 0.5340 | Val AUC: 0.7270 \n",
      "Epoch 188 | Loss: 0.5335 | Val AUC: 0.7257 \n",
      "Epoch 189 | Loss: 0.5338 | Val AUC: 0.7277 \n",
      "Epoch 190 | Loss: 0.5334 | Val AUC: 0.7278 \n",
      "Epoch 191 | Loss: 0.5338 | Val AUC: 0.7279 \n",
      "Epoch 192 | Loss: 0.5335 | Val AUC: 0.7282 \n",
      "Epoch 193 | Loss: 0.5332 | Val AUC: 0.7270 \n",
      "Epoch 194 | Loss: 0.5336 | Val AUC: 0.7274 \n",
      "Epoch 195 | Loss: 0.5337 | Val AUC: 0.7271 \n",
      "Epoch 196 | Loss: 0.5335 | Val AUC: 0.7273 \n",
      "Epoch 197 | Loss: 0.5333 | Val AUC: 0.7276 \n",
      "Epoch 198 | Loss: 0.5334 | Val AUC: 0.7249 \n",
      "Epoch 199 | Loss: 0.5336 | Val AUC: 0.7278 \n",
      "Epoch 200 | Loss: 0.5337 | Val AUC: 0.7267 \n",
      "Epoch 201 | Loss: 0.5339 | Val AUC: 0.7272 \n",
      "Epoch 202 | Loss: 0.5331 | Val AUC: 0.7255 \n",
      "Epoch 203 | Loss: 0.5332 | Val AUC: 0.7262 \n",
      "Epoch 204 | Loss: 0.5333 | Val AUC: 0.7249 \n",
      "Epoch 205 | Loss: 0.5332 | Val AUC: 0.7230 \n",
      "Epoch 206 | Loss: 0.5335 | Val AUC: 0.7231 \n",
      "Epoch 207 | Loss: 0.5336 | Val AUC: 0.7255 \n",
      "Epoch 208 | Loss: 0.5332 | Val AUC: 0.7231 \n",
      "Epoch 209 | Loss: 0.5333 | Val AUC: 0.7224 \n",
      "Epoch 210 | Loss: 0.5331 | Val AUC: 0.7232 \n",
      "Epoch 211 | Loss: 0.5328 | Val AUC: 0.7246 \n",
      "Epoch 212 | Loss: 0.5339 | Val AUC: 0.7244 \n",
      "Epoch 213 | Loss: 0.5336 | Val AUC: 0.7268 \n",
      "Epoch 214 | Loss: 0.5331 | Val AUC: 0.7278 \n",
      "Epoch 215 | Loss: 0.5334 | Val AUC: 0.7254 \n",
      "Epoch 216 | Loss: 0.5331 | Val AUC: 0.7265 \n",
      "Epoch 217 | Loss: 0.5335 | Val AUC: 0.7293 \n",
      "Epoch 218 | Loss: 0.5334 | Val AUC: 0.7267 \n",
      "Epoch 219 | Loss: 0.5333 | Val AUC: 0.7271 \n",
      "Epoch 220 | Loss: 0.5333 | Val AUC: 0.7253 \n",
      "Epoch 221 | Loss: 0.5330 | Val AUC: 0.7265 \n",
      "Epoch 222 | Loss: 0.5331 | Val AUC: 0.7303 \n",
      "Epoch 223 | Loss: 0.5333 | Val AUC: 0.7277 \n",
      "Epoch 224 | Loss: 0.5331 | Val AUC: 0.7276 \n",
      "Epoch 225 | Loss: 0.5328 | Val AUC: 0.7267 \n",
      "Epoch 226 | Loss: 0.5327 | Val AUC: 0.7293 \n",
      "Epoch 227 | Loss: 0.5331 | Val AUC: 0.7297 \n",
      "Epoch 228 | Loss: 0.5333 | Val AUC: 0.7298 \n",
      "Epoch 229 | Loss: 0.5333 | Val AUC: 0.7278 \n",
      "Epoch 230 | Loss: 0.5333 | Val AUC: 0.7290 \n",
      "Epoch 231 | Loss: 0.5328 | Val AUC: 0.7299 \n",
      "Epoch 232 | Loss: 0.5333 | Val AUC: 0.7298 \n",
      "Epoch 233 | Loss: 0.5338 | Val AUC: 0.7301 \n",
      "Epoch 234 | Loss: 0.5331 | Val AUC: 0.7275 \n",
      "Epoch 235 | Loss: 0.5332 | Val AUC: 0.7301 \n",
      "Epoch 236 | Loss: 0.5329 | Val AUC: 0.7263 \n",
      "Epoch 237 | Loss: 0.5327 | Val AUC: 0.7272 \n",
      "Epoch 238 | Loss: 0.5331 | Val AUC: 0.7272 \n",
      "Epoch 239 | Loss: 0.5328 | Val AUC: 0.7250 \n",
      "Epoch 240 | Loss: 0.5335 | Val AUC: 0.7298 \n",
      "Epoch 241 | Loss: 0.5330 | Val AUC: 0.7302 \n",
      "Epoch 242 | Loss: 0.5333 | Val AUC: 0.7265 \n",
      "Epoch 243 | Loss: 0.5329 | Val AUC: 0.7272 \n",
      "Epoch 244 | Loss: 0.5332 | Val AUC: 0.7294 \n",
      "Epoch 245 | Loss: 0.5329 | Val AUC: 0.7296 \n",
      "Epoch 246 | Loss: 0.5334 | Val AUC: 0.7291 \n",
      "Epoch 247 | Loss: 0.5337 | Val AUC: 0.7295 \n",
      "Epoch 248 | Loss: 0.5329 | Val AUC: 0.7295 \n",
      "Epoch 249 | Loss: 0.5330 | Val AUC: 0.7296 \n",
      "Epoch 250 | Loss: 0.5325 | Val AUC: 0.7295 \n",
      "Epoch 251 | Loss: 0.5327 | Val AUC: 0.7283 \n",
      "Epoch 252 | Loss: 0.5325 | Val AUC: 0.7282 \n",
      "Epoch 253 | Loss: 0.5328 | Val AUC: 0.7277 \n",
      "Epoch 254 | Loss: 0.5327 | Val AUC: 0.7280 \n",
      "Epoch 255 | Loss: 0.5327 | Val AUC: 0.7263 \n",
      "Epoch 256 | Loss: 0.5327 | Val AUC: 0.7261 \n",
      "Epoch 257 | Loss: 0.5328 | Val AUC: 0.7265 \n",
      "Epoch 258 | Loss: 0.5332 | Val AUC: 0.7238 \n",
      "Epoch 259 | Loss: 0.5324 | Val AUC: 0.7245 \n",
      "Epoch 260 | Loss: 0.5325 | Val AUC: 0.7262 \n",
      "Epoch 261 | Loss: 0.5329 | Val AUC: 0.7269 \n",
      "Epoch 262 | Loss: 0.5329 | Val AUC: 0.7270 \n",
      "Epoch 263 | Loss: 0.5328 | Val AUC: 0.7249 \n",
      "Epoch 264 | Loss: 0.5329 | Val AUC: 0.7249 \n",
      "Epoch 265 | Loss: 0.5330 | Val AUC: 0.7271 \n",
      "Epoch 266 | Loss: 0.5331 | Val AUC: 0.7266 \n",
      "Epoch 267 | Loss: 0.5323 | Val AUC: 0.7240 \n",
      "Epoch 268 | Loss: 0.5327 | Val AUC: 0.7274 \n",
      "Epoch 269 | Loss: 0.5327 | Val AUC: 0.7271 \n",
      "Epoch 270 | Loss: 0.5331 | Val AUC: 0.7257 \n",
      "Epoch 271 | Loss: 0.5329 | Val AUC: 0.7258 \n",
      "Epoch 272 | Loss: 0.5330 | Val AUC: 0.7255 \n",
      "Epoch 273 | Loss: 0.5324 | Val AUC: 0.7260 \n",
      "Epoch 274 | Loss: 0.5322 | Val AUC: 0.7253 \n",
      "Epoch 275 | Loss: 0.5325 | Val AUC: 0.7267 \n",
      "Epoch 276 | Loss: 0.5327 | Val AUC: 0.7256 \n",
      "Epoch 277 | Loss: 0.5330 | Val AUC: 0.7265 \n",
      "Epoch 278 | Loss: 0.5322 | Val AUC: 0.7250 \n",
      "Epoch 279 | Loss: 0.5322 | Val AUC: 0.7244 \n",
      "Epoch 280 | Loss: 0.5327 | Val AUC: 0.7241 \n",
      "Epoch 281 | Loss: 0.5323 | Val AUC: 0.7264 \n",
      "Epoch 282 | Loss: 0.5321 | Val AUC: 0.7258 \n",
      "Epoch 283 | Loss: 0.5325 | Val AUC: 0.7230 \n",
      "Epoch 284 | Loss: 0.5328 | Val AUC: 0.7243 \n",
      "Epoch 285 | Loss: 0.5329 | Val AUC: 0.7207 \n",
      "Epoch 286 | Loss: 0.5320 | Val AUC: 0.7204 \n",
      "Epoch 287 | Loss: 0.5322 | Val AUC: 0.7206 \n",
      "Epoch 288 | Loss: 0.5336 | Val AUC: 0.7206 \n",
      "Epoch 289 | Loss: 0.5327 | Val AUC: 0.7191 \n",
      "Epoch 290 | Loss: 0.5324 | Val AUC: 0.7240 \n",
      "Epoch 291 | Loss: 0.5322 | Val AUC: 0.7240 \n",
      "Epoch 292 | Loss: 0.5321 | Val AUC: 0.7237 \n",
      "Epoch 293 | Loss: 0.5323 | Val AUC: 0.7236 \n",
      "Epoch 294 | Loss: 0.5331 | Val AUC: 0.7209 \n",
      "Epoch 295 | Loss: 0.5325 | Val AUC: 0.7210 \n",
      "Epoch 296 | Loss: 0.5326 | Val AUC: 0.7233 \n",
      "Epoch 297 | Loss: 0.5324 | Val AUC: 0.7233 \n",
      "Epoch 298 | Loss: 0.5321 | Val AUC: 0.7245 \n",
      "Epoch 299 | Loss: 0.5320 | Val AUC: 0.7238 \n",
      "Test AUC: 0.7604 | Test Accuracy: 0.7462 | Test F1: 0.7128 | Test Precision: 0.8207 | Test Recall: 0.6300\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss = []\n",
    "for epoch in range(300):\n",
    "    loss = train()\n",
    "    val_results = test(val_loader)\n",
    "    val_auc, val_accuracy, val_f1, val_precision, val_recall = val_results\n",
    "    print(f'Epoch {epoch:>2} | Loss: {loss:.4f} | Val AUC: {val_auc:.4f} ')\n",
    "    train_loss.append(loss)\n",
    "\n",
    "test_results = test(test_loader)\n",
    "test_auc, test_accuracy, test_f1, test_precision, test_recall = test_results \n",
    "print(f'Test AUC: {test_auc:.4f} | Test Accuracy: {test_accuracy:.4f} | Test F1: {test_f1:.4f} | Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9171997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece6569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
